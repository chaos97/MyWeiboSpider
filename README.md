# WeiboSpiderThis is a sina weibo spider built by scrapy# mater_proxy分支本分支在,master分支上增加了ip代理池。ip代理池模块在proxy文件夹下，代理来自讯代理的付费优质代理。代理池启动后会通过讯代理提供的API接口获取ip，赋值初始分数后存入redis有序集合中，并按照一定的周期检验ip的可用性进行分数的变动，用于检验的网址是https://weibo.cn，可在proxy/proxypool/setting.py中进行修改。ip代理模块中使用Flask框架写了一个获取随机ip地址的API接口，通过访问http://localhost:5555/random就可随机返回代理池中一个ip。此外，在middleware中增加了ProxyMiddleware用于改变请求失败的request的ip。当一个request产生至被送到spider爬取之前，先经过CookieMiddleware随机使用数据库中的一个cookie，再经过ProxyMiddleware随机使用一个ip地址。---该项目爬取的数据字段说明，请移步:[数据字段说明与示例](./data_stracture.md)## 如何使用### 克隆本项目 && 安装依赖本项目Python版本为Python3.6```bashgit clone git@github.com:chaos97/MyWeiboSpider.gitcd WeiboSpiderpip install -r requirements.txt```除此之外，还需要安装mongodb，phantomjs和redis### 购买账号小号购买地址(**访问需要翻墙**): http://www.xiaohao.shop/ 需要购买**绑号无验证码类型的微博小号**（重点！）![](./images/xiaohao_shop.png)购买越多，sina/settings.py 中的延迟就可以越低，并发也就可以越大**将购买的账号复制到`sina/account_build/account.txt`中，格式与`account_sample.txt`保持一致**。### 构建账号池```bashpython sina/account_build/login.py```运行截图:![](./images/account_build_screenshot.png)这时你的mongodb中将多一个账号表，如下所示:![](./images/account.png)### 构建ip池``` bashpython sina/proxy/run.py```保持该cmd窗口持续运行（会动态获取、检测ip并剔除失效ip）可以通过访问http://localhost:5555/count来查看当前ip池中的ip数量### 运行爬虫不要关闭ip池的cmd窗口！！！新建一个cmd窗口运行爬虫！！！```bashscrapy crawl weibo_spider ```运行截图:![](./images/spider.png)导入pycharm后，也可以直接执行`sina/spider/weibo_spider.py`该爬虫是示例爬虫，将爬取 回忆专用小马甲 ， 王可可是个碧池 等的 用户信息，全部微博，每条微博的评论，还有用户关系。可以根据你的实际需求改写示例爬虫。---爬虫实际速度和你自己电脑的网速/CPU/内存有很大关系。